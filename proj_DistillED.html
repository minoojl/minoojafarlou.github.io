<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Adversarially-Distilled Event Detection</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <main class="content">

    <hr>
    <h1>Enhanced Event Detection through Adversarial Training and Open-Domain Triggers</h1>
    <p><strong>Description:</strong> Event detection (ED) remains a critical challenge in natural language processing, particularly in scenarios with limited labeled data and sparsely labeled or unseen triggers. To address these issues, we propose a unified framework that integrates adversarial training and knowledge distillation, combining the strengths of two complementary approaches: adversarial denoising of weakly supervised data and open-domain trigger knowledge distillation. Our framework utilizes a generator-discriminator module to filter noisy event instances while preserving rare triggers and incorporates external lexical knowledge to enrich trigger identification and semantic understanding. By embedding open-domain knowledge into the adversarial training process, we enhance the generator’s ability to identify unseen triggers and refine the discriminator’s robustness. Additionally, a teacher-student model is employed to distill open-domain knowledge from both labeled and unlabeled corpora, improving model generalization and reducing reliance on densely labeled triggers. Experiments on benchmark datasets demonstrate significant improvements in recall, precision, and cross-domain adaptation, with superior performance in identifying unseen and sparsely labeled triggers. This hybrid approach bridges the gap between weakly supervised learning and knowledge-based generalization, offering a scalable and effective solution for event detection tasks.</p>
  
  </main>
</body>
</html>
